{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "This is the third assignment for the Coursera course \"Advanced Machine Learning and Signal Processing\"\n\nAgain, please insert to code to your ApacheCouchDB based Cloudant instance below using the \"Insert Code\" function of Watson Studio (you've done this in Assignment 1 and 2 before)\n\nDone, just execute all cells one after the other and you are done - just note that in the last one you must update your email address (the one you've used for coursera) and obtain a submission token, you get this from the programming assignment directly on coursera.\n\nPlease fill in the sections labelled with \"###YOUR_CODE_GOES_HERE###\"\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "Let's create a SparkSession object and put the Cloudant credentials into it", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark = SparkSession\\\n    .builder\\\n    .appName(\"Cloudant Spark SQL Example in Python using temp tables\")\\\n    .config(\"cloudant.host\",credentials_1['custom_url'].split('@')[1])\\\n    .config(\"cloudant.username\", credentials_1['username'])\\\n    .config(\"cloudant.password\",credentials_1['password'])\\\n    .getOrCreate()"
        }, 
        {
            "source": "Now it\u2019s time to have a look at the recorded sensor data. You should see data similar to the one exemplified below\u2026.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+--------+----+----+----+--------------------+--------------------+\n|CLASS|SENSORID|   X|   Y|   Z|                 _id|                _rev|\n+-----+--------+----+----+----+--------------------+--------------------+\n|    0|asdfghjk|0.56|0.56|0.56|004d76e2b1a0032fb...|1-299af9451269e3f...|\n|    0|asdfghjk|0.48|0.48|0.48|004d76e2b1a0032fb...|1-a6293726be13827...|\n|    0|asdfghjk|0.52|0.52|0.52|004d76e2b1a0032fb...|1-53df1f5881f49c6...|\n|    0|asdfghjk|0.59|0.59|0.59|004d76e2b1a0032fb...|1-60964f5eedcdb4a...|\n|    0|asdfghjk|0.49|0.49|0.49|004d76e2b1a0032fb...|1-387ae130f05b80d...|\n|    0|asdfghjk|0.39|0.39|0.39|004d76e2b1a0032fb...|1-f7d1e503ce5ee3d...|\n|    0|asdfghjk|0.53|0.53|0.53|004d76e2b1a0032fb...|1-67f271865e8425a...|\n|    1|asdfghjk|0.56|0.56|0.56|004d76e2b1a0032fb...|1-f30bd5308354359...|\n|    1|asdfghjk|0.52|0.52|0.52|004d76e2b1a0032fb...|1-3a9819a97ae307d...|\n|    1|asdfghjk|0.43|0.43|0.43|004d76e2b1a0032fb...|1-6210722f6928f53...|\n|    1|asdfghjk|0.49|0.49|0.49|004d76e2b1a0032fb...|1-3ca6057c3c02c38...|\n|    1|asdfghjk|0.48|0.48|0.48|004d76e2b1a0032fb...|1-e74c203a8926180...|\n|    1|asdfghjk| 0.5| 0.5| 0.5|004d76e2b1a0032fb...|1-aca090dbec7066d...|\n|    1|asdfghjk|0.44|0.44|0.44|004d76e2b1a0032fb...|1-77675c9be36ae3b...|\n|    1|asdfghjk|0.46|0.46|0.46|004d76e2b1a0032fb...|1-bf183ba15cd7060...|\n|    1|asdfghjk|0.57|0.57|0.57|004d76e2b1a0032fb...|1-7e5b1e51d6f6cd9...|\n|    1|asdfghjk|0.52|0.52|0.52|004d76e2b1a0032fb...|1-3a9819a97ae307d...|\n|    1|asdfghjk| 0.5| 0.5| 0.5|004d76e2b1a0032fb...|1-aca090dbec7066d...|\n|    1|asdfghjk|0.57|0.57|0.57|004d76e2b1a0032fb...|1-7e5b1e51d6f6cd9...|\n|    1|asdfghjk|0.48|0.48|0.48|004d76e2b1a0032fb...|1-e74c203a8926180...|\n+-----+--------+----+----+----+--------------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df=spark.read.load('shake_classification', \"org.apache.bahir.cloudant\")\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()\n"
        }, 
        {
            "source": "Let\u2019s check if we have balanced classes \u2013 this means that we have roughly the same number of examples for each class we want to predict. This is important for classification but also helpful for clustering", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------------+-----+\n|count(class)|class|\n+------------+-----+\n|          72|    0|\n|         187|    1|\n+------------+-----+\n\n"
                }
            ], 
            "source": "spark.sql(\"SELECT count(class), class from df group by class\").show()"
        }, 
        {
            "source": "Let's create a VectorAssembler which consumes columns X, Y and Z and produces a column \u201cfeatures\u201d\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols=[\"X\",\"Y\",\"Z\"],\n                                  outputCol=\"features\")"
        }, 
        {
            "source": "Please insatiate a clustering algorithm from the SparkML package and assign it to the clust variable. Here we don\u2019t need to take care of the \u201cCLASS\u201d column since we are in unsupervised learning mode \u2013 so let\u2019s pretend to not even have the \u201cCLASS\u201d column for now \u2013 but it will become very handy later in assessing the clustering performance. PLEASE NOTE \u2013 IN REAL-WORLD SCENARIOS THERE IS NO CLASS COLUMN \u2013 THEREFORE YOU CAN\u2019T ASSESS CLASSIFICATION PERFORMANCE USING THIS COLUMN \n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Help on class KMeans in module pyspark.ml.clustering:\n\nclass KMeans(pyspark.ml.wrapper.JavaEstimator, pyspark.ml.param.shared.HasFeaturesCol, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasMaxIter, pyspark.ml.param.shared.HasTol, pyspark.ml.param.shared.HasSeed, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n |  K-means clustering with a k-means++ like initialization mode\n |  (the k-means|| algorithm by Bahmani et al).\n |  \n |  >>> from pyspark.ml.linalg import Vectors\n |  >>> data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),\n |  ...         (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n |  >>> df = spark.createDataFrame(data, [\"features\"])\n |  >>> kmeans = KMeans(k=2, seed=1)\n |  >>> model = kmeans.fit(df)\n |  >>> centers = model.clusterCenters()\n |  >>> len(centers)\n |  2\n |  >>> model.computeCost(df)\n |  2.000...\n |  >>> transformed = model.transform(df).select(\"features\", \"prediction\")\n |  >>> rows = transformed.collect()\n |  >>> rows[0].prediction == rows[1].prediction\n |  True\n |  >>> rows[2].prediction == rows[3].prediction\n |  True\n |  >>> model.hasSummary\n |  True\n |  >>> summary = model.summary\n |  >>> summary.k\n |  2\n |  >>> summary.clusterSizes\n |  [2, 2]\n |  >>> kmeans_path = temp_path + \"/kmeans\"\n |  >>> kmeans.save(kmeans_path)\n |  >>> kmeans2 = KMeans.load(kmeans_path)\n |  >>> kmeans2.getK()\n |  2\n |  >>> model_path = temp_path + \"/kmeans_model\"\n |  >>> model.save(model_path)\n |  >>> model2 = KMeansModel.load(model_path)\n |  >>> model2.hasSummary\n |  False\n |  >>> model.clusterCenters()[0] == model2.clusterCenters()[0]\n |  array([ True,  True], dtype=bool)\n |  >>> model.clusterCenters()[1] == model2.clusterCenters()[1]\n |  array([ True,  True], dtype=bool)\n |  \n |  .. versionadded:: 1.5.0\n |  \n |  Method resolution order:\n |      KMeans\n |      pyspark.ml.wrapper.JavaEstimator\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      pyspark.ml.param.shared.HasFeaturesCol\n |      pyspark.ml.param.shared.HasPredictionCol\n |      pyspark.ml.param.shared.HasMaxIter\n |      pyspark.ml.param.shared.HasTol\n |      pyspark.ml.param.shared.HasSeed\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, featuresCol='features', predictionCol='prediction', k=2, initMode='k-means||', initSteps=2, tol=0.0001, maxIter=20, seed=None)\n |      __init__(self, featuresCol=\"features\", predictionCol=\"prediction\", k=2,                  initMode=\"k-means||\", initSteps=2, tol=1e-4, maxIter=20, seed=None)\n |  \n |  getInitMode(self)\n |      Gets the value of `initMode`\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  getInitSteps(self)\n |      Gets the value of `initSteps`\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  getK(self)\n |      Gets the value of `k`\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  setInitMode(self, value)\n |      Sets the value of :py:attr:`initMode`.\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  setInitSteps(self, value)\n |      Sets the value of :py:attr:`initSteps`.\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  setK(self, value)\n |      Sets the value of :py:attr:`k`.\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  setParams(self, featuresCol='features', predictionCol='prediction', k=2, initMode='k-means||', initSteps=2, tol=0.0001, maxIter=20, seed=None)\n |      setParams(self, featuresCol=\"features\", predictionCol=\"prediction\", k=2,                   initMode=\"k-means||\", initSteps=2, tol=1e-4, maxIter=20, seed=None)\n |      \n |      Sets params for KMeans.\n |      \n |      .. versionadded:: 1.5.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  initMode = Param(parent='undefined', name='initMode', doc='...means||\"...\n |  \n |  initSteps = Param(parent='undefined', name='initSteps', doc=...for k-m...\n |  \n |  k = Param(parent='undefined', name='k', doc='The number of clusters to...\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n |  \n |  __metaclass__ = <class 'abc.ABCMeta'>\n |      Metaclass for defining Abstract Base Classes (ABCs).\n |      \n |      Use this metaclass to create an ABC.  An ABC can be subclassed\n |      directly, and then acts as a mix-in class.  You can also register\n |      unrelated concrete classes (even built-in classes) and unrelated\n |      ABCs as 'virtual subclasses' -- these and their descendants will\n |      be considered subclasses of the registering ABC by the built-in\n |      issubclass() function, but the registering ABC won't show up in\n |      their MRO (Method Resolution Order) nor will method\n |      implementations defined by the registering ABC be callable (not\n |      even via super()).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  __del__(self)\n |  \n |  copy(self, extra=None)\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      :param extra: Extra parameters to copy to the new instance\n |      :return: Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset, params=None)\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n |                     param maps is given, this calls fit on each param map and returns a list of\n |                     models.\n |      :returns: fitted model(s)\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  getFeaturesCol(self)\n |      Gets the value of featuresCol or its default value.\n |  \n |  setFeaturesCol(self, value)\n |      Sets the value of :py:attr:`featuresCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  getPredictionCol(self)\n |      Gets the value of predictionCol or its default value.\n |  \n |  setPredictionCol(self, value)\n |      Sets the value of :py:attr:`predictionCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n |  \n |  getMaxIter(self)\n |      Gets the value of maxIter or its default value.\n |  \n |  setMaxIter(self, value)\n |      Sets the value of :py:attr:`maxIter`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n |  \n |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasTol:\n |  \n |  getTol(self)\n |      Gets the value of tol or its default value.\n |  \n |  setTol(self, value)\n |      Sets the value of :py:attr:`tol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n |  \n |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  getSeed(self)\n |      Gets the value of seed or its default value.\n |  \n |  setSeed(self, value)\n |      Sets the value of :py:attr:`seed`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param)\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  explainParams(self)\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  extractParamMap(self, extra=None)\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values <\n |      user-supplied values < extra.\n |      \n |      :param extra: extra param values\n |      :return: merged param map\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getOrDefault(self, param)\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getParam(self, paramName)\n |      Gets a param by its name.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  hasDefault(self, param)\n |      Checks whether a param has a default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  hasParam(self, paramName)\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  isDefined(self, param)\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  isSet(self, param)\n |      Checks whether a param is explicitly set by user.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self)\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path)\n |      Save this ML instance to the given path, a shortcut of `write().save(path)`.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n |  \n |  read() from builtins.type\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path) from builtins.type\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n\n"
                }
            ], 
            "source": "from pyspark.ml.clustering import KMeans\nhelp(KMeans)\nclust = KMeans(k=20, initMode=\"random\")"
        }, 
        {
            "source": "Let\u2019s train...\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, clust])\nmodel = pipeline.fit(df)"
        }, 
        {
            "source": "...and evaluate...", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+--------+----+----+----+--------------------+--------------------+----------------+----------+\n|CLASS|SENSORID|   X|   Y|   Z|                 _id|                _rev|        features|prediction|\n+-----+--------+----+----+----+--------------------+--------------------+----------------+----------+\n|    0|asdfghjk|0.56|0.56|0.56|004d76e2b1a0032fb...|1-299af9451269e3f...|[0.56,0.56,0.56]|         6|\n|    0|asdfghjk|0.48|0.48|0.48|004d76e2b1a0032fb...|1-a6293726be13827...|[0.48,0.48,0.48]|         0|\n|    0|asdfghjk|0.52|0.52|0.52|004d76e2b1a0032fb...|1-53df1f5881f49c6...|[0.52,0.52,0.52]|         4|\n|    0|asdfghjk|0.59|0.59|0.59|004d76e2b1a0032fb...|1-60964f5eedcdb4a...|[0.59,0.59,0.59]|         9|\n|    0|asdfghjk|0.49|0.49|0.49|004d76e2b1a0032fb...|1-387ae130f05b80d...|[0.49,0.49,0.49]|        10|\n|    0|asdfghjk|0.39|0.39|0.39|004d76e2b1a0032fb...|1-f7d1e503ce5ee3d...|[0.39,0.39,0.39]|         1|\n|    0|asdfghjk|0.53|0.53|0.53|004d76e2b1a0032fb...|1-67f271865e8425a...|[0.53,0.53,0.53]|         2|\n|    1|asdfghjk|0.56|0.56|0.56|004d76e2b1a0032fb...|1-f30bd5308354359...|[0.56,0.56,0.56]|         6|\n|    1|asdfghjk|0.52|0.52|0.52|004d76e2b1a0032fb...|1-3a9819a97ae307d...|[0.52,0.52,0.52]|         4|\n|    1|asdfghjk|0.43|0.43|0.43|004d76e2b1a0032fb...|1-6210722f6928f53...|[0.43,0.43,0.43]|         5|\n|    1|asdfghjk|0.49|0.49|0.49|004d76e2b1a0032fb...|1-3ca6057c3c02c38...|[0.49,0.49,0.49]|        10|\n|    1|asdfghjk|0.48|0.48|0.48|004d76e2b1a0032fb...|1-e74c203a8926180...|[0.48,0.48,0.48]|         0|\n|    1|asdfghjk| 0.5| 0.5| 0.5|004d76e2b1a0032fb...|1-aca090dbec7066d...|   [0.5,0.5,0.5]|         3|\n|    1|asdfghjk|0.44|0.44|0.44|004d76e2b1a0032fb...|1-77675c9be36ae3b...|[0.44,0.44,0.44]|         5|\n|    1|asdfghjk|0.46|0.46|0.46|004d76e2b1a0032fb...|1-bf183ba15cd7060...|[0.46,0.46,0.46]|         8|\n|    1|asdfghjk|0.57|0.57|0.57|004d76e2b1a0032fb...|1-7e5b1e51d6f6cd9...|[0.57,0.57,0.57]|         6|\n|    1|asdfghjk|0.52|0.52|0.52|004d76e2b1a0032fb...|1-3a9819a97ae307d...|[0.52,0.52,0.52]|         4|\n|    1|asdfghjk| 0.5| 0.5| 0.5|004d76e2b1a0032fb...|1-aca090dbec7066d...|   [0.5,0.5,0.5]|         3|\n|    1|asdfghjk|0.57|0.57|0.57|004d76e2b1a0032fb...|1-7e5b1e51d6f6cd9...|[0.57,0.57,0.57]|         6|\n|    1|asdfghjk|0.48|0.48|0.48|004d76e2b1a0032fb...|1-e74c203a8926180...|[0.48,0.48,0.48]|         0|\n+-----+--------+----+----+----+--------------------+--------------------+----------------+----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "prediction = model.transform(df)\nprediction.show()"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 30, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9266409266409267"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "prediction.createOrReplaceTempView('prediction')\nspark.sql('''\nselect max(correct)/max(total) as accuracy from (\n\n    select sum(correct) as correct, count(correct) as total from (\n        select case when class != prediction then 1 else 0 end as correct from prediction \n    ) \n    \n    union\n    \n    select sum(correct) as correct, count(correct) as total from (\n        select case when class = prediction then 1 else 0 end as correct from prediction \n    ) \n)\n''').rdd.map(lambda row: row.accuracy).collect()[0]"
        }, 
        {
            "source": "If you reached at least 55% of accuracy you are fine to submit your predictions to the grader. Otherwise please experiment with parameters setting to your clustering algorithm, use a different algorithm or just re-record your data and try to obtain. In case you are stuck, please use the Coursera Discussion Forum. Please note again \u2013 in a real-world scenario there is no way in doing this \u2013 since there is no class label in your data. Please have a look at this further reading on clustering performance evaluation https://en.wikipedia.org/wiki/Cluster_analysis#Evaluation_and_assessment\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!rm -Rf a2_m3.json"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-08-27 10:08:42--  https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2029 (2.0K) [text/plain]\nSaving to: \u2018rklib.py\u2019\n\n100%[======================================>] 2,029       --.-K/s   in 0s      \n\n2018-08-27 10:08:43 (17.6 MB/s) - \u2018rklib.py\u2019 saved [2029/2029]\n\n"
                }
            ], 
            "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py"
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction= prediction.repartition(1)\nprediction.write.json('a2_m3.json')"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "  adding: a2_m3.json/ (stored 0%)\r\n  adding: a2_m3.json/_SUCCESS (stored 0%)\r\n  adding: a2_m3.json/part-00000-fefa531d-e7ba-46ed-ba61-4eb76eb82863.json (deflated 92%)\r\n  adding: a2_m3.json/._SUCCESS.crc (stored 0%)\r\n  adding: a2_m3.json/.part-00000-fefa531d-e7ba-46ed-ba61-4eb76eb82863.json.crc (stored 0%)\r\n"
                }
            ], 
            "source": "!zip -r a2_m3.json.zip a2_m3.json"
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!base64 a2_m3.json.zip > a2_m3.json.zip.base64"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Submission successful, please check on the coursera grader page for the status\n"
                }
            ], 
            "source": "from rklib import submit\nkey = \"pPfm62VXEeiJOBL0dhxPkA\"\npart = \"EOTMs\"\nemail = \"felipebihaiek@gmail.com\"\nsecret = \"yhVlZIARf7MdERs3\"\n\nwith open('a2_m3.json.zip.base64', 'r') as myfile:\n    data=myfile.read()\nsubmit(email, secret, key, part, [part], data)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}